{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Workshop III - Intro to Machine Learning\n",
    "\n",
    "Machine learning is a rapidly growing application that is being applied accross a multitude of fields. Boiled down to its simplist explanation, machine learning is generating models or a series of models that predict values that do not exist yet. In other words, how do the attributes of something compose its value and how can we model those attributes to predict values of that something that do not exist yet?\n",
    "\n",
    "A common application to introduce machine learning is hedonic pricing, or how do the attributes of something determine its price?\n",
    "\n",
    "In this exercise we will be looking at attributes (bedrooms, number of allowable guests, etc.,) of AirBnBs in Boston and how those attributes determine the price of listings. Once we develop a model that accurately predicts existing listings, we will then use those estimated values to predict the values of listings based on attributes we have identified.\n",
    "\n",
    "Accoding to Data Quest, machine learning models perform the following tasks in regards to an application such as valuating real estate:\n",
    "\n",
    "1. Examining a large data set of past home sales (observations)\n",
    "\n",
    "\n",
    "2. Finding patterns and statistical relationships between a house’s characteristics (features) and its price (the target variable), including patterns that might not be evident to a human who’s looking at the data\n",
    "\n",
    "\n",
    "3. Using these statistical relationships and patterns to predict the price of any new houses we feed it data on.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "The content for this workshop comes from the following sources:\n",
    "\n",
    "## Lesson Content - Data Quest\n",
    "\n",
    "https://www.dataquest.io/blog/machine-learning-tutorial/\n",
    "\n",
    "## Data for AirBnB Listings\n",
    "\n",
    "http://insideairbnb.com/get-the-data.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manipulate our data and import our data, we will be using the Pandas package. This is a very common package predominantly used in data science and social science applications in Python (i.e., economics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston_listings = pd.read_csv(r'filepath\\boston_airbnb.csv')\n",
    "print(boston_listings.shape)\n",
    "boston_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove variables that are not needed / keep ones we want\n",
    "boston_listings = boston_listings[['host_response_rate','host_acceptance_rate', 'host_listings_count','accommodates','room_type','bedrooms','bathrooms','beds','price','cleaning_fee','security_deposit','minimum_nights','maximum_nights','number_of_reviews','latitude','longitude','city','zipcode','state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## What are the dimensions of a our new data frame? We will need to take note of the number of observations for later\n",
    "print(boston_listings.shape)\n",
    "boston_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The K-nearest neighbors algorithm\n",
    "\n",
    "The k-nearest neighbors algorithm stores all AirBnB listing and classifies each listing based on its similarity to another listing. Similarity is based on variables of interest and their euclidian distance (How far apart are they numerically).\n",
    "\n",
    "To perfrom our K-nearest neighbors algorithm we:\n",
    "\n",
    "First, we select the number of similar listings k, that we want to compare with.\n",
    "\n",
    "Second, we need to calculate how similar each listing is to ours using a similarity metric.\n",
    "\n",
    "Third, we rank each listing using our similarity metric and select the first k listings.\n",
    "\n",
    "Finally, we calculate the mean price for the k similar listings, and use that as our list price.\n",
    "\n",
    "\n",
    "Let’s start building our real model by defining the similarity metric we’re going to use. Then, we’ll implement the k-nearest neighbors algorithm and use it to suggest a price for a new listing. For the purposes of this tutorial, we’re going to use a fixed k value of 5, but once you become familiar with the workflow of the algorithm you can experiment with this value to see if you get better results with lower or higher k values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this cell we are going to calculate the distance between the first living\n",
    "## space in the data set and our own. The smallest distance we can achieve is 0\n",
    "\n",
    "import numpy as np\n",
    "our_acc_value = 3\n",
    "first_living_space_value = boston_listings.loc[0,'accommodates']\n",
    "first_distance = np.abs(first_living_space_value - our_acc_value)\n",
    "print(first_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, we are going to calculate all of the distance for each observations\n",
    "## in the data set and tabulate it. As you can see, we have 429 observations with\n",
    "## a value of 0. This that these 429 observations are similar to each other.\n",
    "## Let's use them for our machine learning\n",
    "\n",
    "boston_listings['distance'] = np.abs(boston_listings.accommodates - our_acc_value)\n",
    "boston_listings.distance.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If we were to use our data sequentially, that would introdue bias into our\n",
    "## our calculations. So, we are going to randomize our data\n",
    "\n",
    "boston_listings = boston_listings.sample(frac=1,random_state=0)\n",
    "boston_listings = boston_listings.sort_values('distance')\n",
    "boston_listings.price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See how our price has a dollar sign in it? We need to remove that to make\n",
    "## it a value that python can work with.\n",
    "\n",
    "boston_listings['price'] = boston_listings.price.str.replace(\"\\$|,\",'').astype(float)\n",
    "mean_price = boston_listings.price.iloc[:5].mean()\n",
    "mean_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now made our first prediction. Our KNN model has told us that when we use the accomodates deature to predict price, we get an average price of $168.80 for a three-person listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a cool result, we do not know how accurate this result is. So, next we are going to 'train' our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we are going to split it into 2 partions with one group holding 75% of our data and another group holding 25% of our data.\n",
    "\n",
    "The rows in the training set (train_df) are used to predict the price value for the rows in the test set\n",
    "\n",
    "We then compare the predicted values with the actual price values in the test to see how accurate they are\n",
    "\n",
    "We are also going to drop our 'distance' variable we generated before to make a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_listings = boston_listings[['host_response_rate','host_acceptance_rate', 'host_listings_count','accommodates','room_type','bedrooms','bathrooms','beds','price','cleaning_fee','security_deposit','minimum_nights','maximum_nights','number_of_reviews','latitude','longitude','city','zipcode','state']]\n",
    "train_df = boston_listings.copy().iloc[:2630]\n",
    "test_df = boston_listings.copy().iloc[2630:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier for ourselves while we look at metrics, we’ll combine the model we made earlier into a function. We won’t need to worry about randomizing the rows, since they’re still randomized from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(new_listing_value,feature_column):\n",
    "    temp_df = train_df\n",
    "    temp_df['distance'] = np.abs(boston_listings[feature_column] - new_listing_value)\n",
    "    temp_df = temp_df.sort_values('distance')\n",
    "    knn_5 = temp_df.price.iloc[:5]\n",
    "    predicted_price = knn_5.mean()\n",
    "    return(predicted_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function to predict values for our test dataset using the accommodates column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predicted_price'] = test_df.accommodates.apply(predict_price,feature_column='accommodates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RMSE to Evaluate Our Model\n",
    "\n",
    "For many prediction tasks, we want to penalize predicted values that are further away from the actual value much more than those that are closer to the actual value.\n",
    "\n",
    "To do this, we can take the mean of the squared error values, which is called the root mean squared error (RMSE). Here’s the formula for RMSE:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Root-mean-square_deviation\n",
    "\n",
    "where n represents the number of rows in the test set. This formula might look overwhelming at first, but all we’re doing is:\n",
    "\n",
    "- Taking the difference between each predicted value and the actual value (or error),\n",
    "\n",
    "- Squaring this difference (square),\n",
    "\n",
    "- Taking the mean of all the squared differences (mean), and\n",
    "\n",
    "- Taking the square root of that mean (root).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['squared_error'] = (test_df['predicted_price'] - test_df['price'])**(2)\n",
    "mse = test_df['squared_error'].mean()\n",
    "rmse = mse ** (1/2)\n",
    "print('Our Root Mean Square Error is:', round(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller the RMSE the better so, a value of 182 is not great.\n",
    "\n",
    "Of some of our variables, let us calculate the RMSE for each to see which one has the smallest value. We can use that one for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['accommodates','bedrooms','bathrooms','number_of_reviews']:\n",
    "    test_df['predicted_price'] = test_df.accommodates.apply(predict_price,feature_column=feature)\n",
    "    test_df['squared_error'] = (test_df['predicted_price'] - test_df['price'])**(2)\n",
    "    mse = test_df['squared_error'].mean()\n",
    "    rmse = mse ** (1/2)\n",
    "    print(\"RMSE for the {} column: {}\".format(feature,rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 'bedrooms' and 'accomodate' variables have the lowest RMSE. They are still not great though.\n",
    "\n",
    "HOWEVER\n",
    "\n",
    "We can minimize this error by incorporating additional variables into our prediction, or going from a univariate model to a multivariate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Variables\n",
    "\n",
    "We’re going to read in a cleaned version of this data set so that we can focus on evaluating the models. In our cleaned data set:\n",
    "\n",
    "- All columns have been converted to numeric values, since we can’t calculate the Euclidean distance of a value with non-numeric characters.\n",
    "- Non numeric columns have been removed for simplicity.\n",
    "- Any listings with missing values have been removed.\n",
    "- We have normalized the columns which will give us more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_listings = pd.read_csv(r'filepath\\boston_airbnb.csv')\n",
    "boston_listings = boston_listings[['accommodates','bedrooms','bathrooms','beds','price','minimum_nights','maximum_nights','number_of_reviews']]\n",
    "boston_listings['price'] = boston_listings.price.str.replace(\"\\$|,\",'').astype(float)\n",
    "print(boston_listings.shape)\n",
    "boston_listings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "a = [boston_listings.accommodates]\n",
    "a = np.nan_to_num(a)\n",
    "b = [boston_listings.bedrooms]\n",
    "b = np.nan_to_num(b)\n",
    "c = [boston_listings.bathrooms]\n",
    "c =np.nan_to_num(c)\n",
    "d = [boston_listings.beds]\n",
    "d = np.nan_to_num(d)\n",
    "e = [boston_listings.price]\n",
    "e = np.nan_to_num(e)\n",
    "f = [boston_listings.minimum_nights]\n",
    "f= np.nan_to_num(f)\n",
    "g = [boston_listings.number_of_reviews]\n",
    "g= np.nan_to_num(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "normalized_a = preprocessing.normalize(a)\n",
    "normalized_b = preprocessing.normalize(b)\n",
    "normalized_c = preprocessing.normalize(c)\n",
    "normalized_d = preprocessing.normalize(d)\n",
    "normalized_e = preprocessing.normalize(e)\n",
    "normalized_f = preprocessing.normalize(f)\n",
    "normalized_g = preprocessing.normalize(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructing the data frame\n",
    "boston_listings[\"accommodates\"].replace({\"normalized_a\"}, inplace=True)\n",
    "boston_listings[\"bedrooms\"].replace({\"normalized_b\"}, inplace=True)\n",
    "boston_listings[\"bathrooms\"].replace({\"normalized_c\"}, inplace=True)\n",
    "boston_listings[\"beds\"].replace({\"normalized_d\"}, inplace=True)\n",
    "boston_listings[\"price\"].replace({\"normalized_e\"}, inplace=True)\n",
    "boston_listings[\"minimum_nights\"].replace({\"normalized_f\"}, inplace=True)\n",
    "boston_listings[\"number_of_reviews\"].replace({\"normalized_g\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data frame to csv to import later\n",
    "boston_listings.to_csv(r'filepath\\boston_airbnb_normalized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_listings = pd.read_csv(r'filepath\\boston_airbnb_normalized.csv')\n",
    "# Importing data frame\n",
    "\n",
    "normalized_listings = normalized_listings.sample(frac=1,random_state=0)\n",
    "# re-randomize observations\n",
    "\n",
    "#Check number of empty observations -- Empty observations make an uneven matrix that cant be processed\n",
    "print(normalized_listings.shape)\n",
    "normalized_listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing missing values with the mean -- you can replace it with whatever though (median, mode, zero, etc.,)\n",
    "\n",
    "bedrooms_mean = normalized_listings['bedrooms'].mean()\n",
    "normalized_listings['bedrooms'].fillna(bedrooms_mean,inplace=True)\n",
    "\n",
    "bathrooms_mean = normalized_listings['bathrooms'].mean()\n",
    "normalized_listings['bathrooms'].fillna(bathrooms_mean,inplace=True)\n",
    "\n",
    "beds_mean = normalized_listings['beds'].mean()\n",
    "normalized_listings['beds'].fillna(beds_mean,inplace=True)\n",
    "\n",
    "normalized_listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerandomizing data and setting training data sets\n",
    "\n",
    "normalized_listings = normalized_listings.sample(frac=1,random_state=0)\n",
    "norm_train_df = normalized_listings.copy().iloc[1:2630]\n",
    "norm_test_df = normalized_listings.copy().iloc[2630:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the distance between the first and fifth listings\n",
    "\n",
    "from scipy.spatial import distance\n",
    "first_listing = normalized_listings.iloc[0][['accommodates', 'bathrooms']]\n",
    "fifth_listing = normalized_listings.iloc[20][['accommodates', 'bathrooms']]\n",
    "first_fifth_distance = distance.euclidean(first_listing, fifth_listing)\n",
    "first_fifth_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculating our RMSE with our new model with TWO predictor variables\n",
    "\n",
    "def predict_price_multivariate(new_listing_value,feature_columns):\n",
    "    temp_df = norm_train_df\n",
    "    temp_df['distance'] = distance.cdist(temp_df[feature_columns],[new_listing_value[feature_columns]])\n",
    "    temp_df = temp_df.sort_values('distance')\n",
    "    knn_5 = temp_df.price.iloc[:5]\n",
    "    predicted_price = knn_5.mean()\n",
    "    return(predicted_price)\n",
    "cols = ['accommodates', 'bathrooms']\n",
    "    \n",
    "\n",
    "norm_test_df['predicted_price'] = norm_test_df[cols].apply(predict_price_multivariate,feature_columns=cols,axis=1)\n",
    "norm_test_df['squared_error'] = (norm_test_df['predicted_price'] - norm_test_df['price'])**(2)\n",
    "mse = norm_test_df['squared_error'].mean()\n",
    "rmse = mse ** (1/2)\n",
    "print(round(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our RMSE for accommodates went from 218 to 113, a pretty good improvement.\n",
    "\n",
    "HOWEVER\n",
    "\n",
    "Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(algorithm='brute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn.fit(train_features, normalized_listings.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions = knn.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using a precanned package that runs much faster\n",
    "\n",
    "knn.fit(norm_train_df[cols],norm_train_df['price'])\n",
    "two_features_predictions = knn.predict(norm_test_df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "two_features_mse = mean_squared_error(norm_test_df['price'], two_features_predictions)\n",
    "two_features_rmse = two_features_mse ** (1/2)\n",
    "print(two_features_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(algorithm='auto')\n",
    "cols = ['accommodates','bedrooms','bathrooms','beds']\n",
    "knn.fit(norm_train_df[cols], norm_train_df['price'])\n",
    "four_features_predictions = knn.predict(norm_test_df[cols])\n",
    "four_features_mse = mean_squared_error(norm_test_df['price'], four_features_predictions)\n",
    "four_features_rmse = four_features_mse ** (1/2)\n",
    "four_features_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running our alternative model seems to have generate worse to similar outcomes. Perhaps if we choose different\n",
    "## variables it would improve?\n",
    "\n",
    "## You can play around with the models by replacing variables you want to include in the models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
